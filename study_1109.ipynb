{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- 이제 학습 데이터의 경로, 클래스 갯수 및 종류가 적혀 있는 yaml 파일 제작을 해야함\\n- 아래는 yolo v5 깃헙에서 제공하는 coco.yaml의 예시\\n- 우리가 수정해야할 부분은 다음과 같다.\\n1. train: 학습 데이타 폴더 경로(이미지)\\n2. val: 검증 데이터 폴더 경로(이미지)\\n3. nc: 학습할 클래스 갯수\\n4. names: 학습할 클래스 이름들\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-6) YOLOv5 데이터셋 만들기1:  yaml 파일 제작\n",
    "\n",
    "'''\n",
    "- 이제 학습 데이터의 경로, 클래스 갯수 및 종류가 적혀 있는 yaml 파일 제작을 해야함\n",
    "- 아래는 yolo v5 깃헙에서 제공하는 coco.yaml의 예시\n",
    "- 우리가 수정해야할 부분은 다음과 같다.\n",
    "1. train: 학습 데이타 폴더 경로(이미지)\n",
    "2. val: 검증 데이터 폴더 경로(이미지)\n",
    "3. nc: 학습할 클래스 갯수\n",
    "4. names: 학습할 클래스 이름들\n",
    "'''\n",
    "\n",
    "# # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
    "# train: ../coco128/images/train2017/\n",
    "# val: ../coco128/images/val2017/\n",
    "\n",
    "# # number of classes\n",
    "# nc: 80\n",
    "\n",
    "# # class names\n",
    "# names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "#         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "#         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "#         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "#         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "#         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "#         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "#         'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \n",
    "#         'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- 욜로 데이터 라벨을 제작해야 함\\n- 아래에 라벨 툴 추천 링크를 남겨두겠다.\\n- (C기반 Darknet yolo v3와 다르게, 이미지 내에 학습할 클래스가 없다면, txt 파일이 필요 없다.)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-6) yolo v5 데이터셋 만들기2: 라벨 txt 파일 제작\n",
    "'''\n",
    "- 욜로 데이터 라벨을 제작해야 함\n",
    "- 아래에 라벨 툴 추천 링크를 남겨두겠다.\n",
    "- (C기반 Darknet yolo v3와 다르게, 이미지 내에 학습할 클래스가 없다면, txt 파일이 필요 없다.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- 다음과 같이 데이터셋 폴더 생성을 함\\n\\n1. 전체 데이터 폴더\\n(1) 이미지 데이터 폴더\\n- train 이미지 데이터 폴더\\n- val 이미지 데이터 폴더\\n\\n(2) 텍스트 라벨 폴더\\n- train 텍스트 라벨 폴더\\n- val 텍스트 라벨 폴더\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 1-6) yolo v5 데이터셋 만들기: 최종\n",
    "'''\n",
    "- 다음과 같이 데이터셋 폴더 생성을 함\n",
    "\n",
    "1. 전체 데이터 폴더\n",
    "(1) 이미지 데이터 폴더\n",
    "- train 이미지 데이터 폴더\n",
    "- val 이미지 데이터 폴더\n",
    "\n",
    "(2) 텍스트 라벨 폴더\n",
    "- train 텍스트 라벨 폴더\n",
    "- val 텍스트 라벨 폴더\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n--data: data yaml 파일 경로 (데이터셋 정보가 적힌 yaml 파일)\\n--weights: Pre-Trained 모델 파일 경로 (pt 형식 파일)\\n\\n- 아무런 값을 적지 않으면('') 랜덤한 weight 값으로 초기화 및 학습 진행\\n- (깃헙 공식 오피셜, pre-train을 추천)\\n- pre-trained 파일이란? : 학습 파라미터(weight와 bias)가 잘 초기화된 파일\\n- pre-trained를 할거면 위 링크에서 다운받은 weight file 이름을 적으면 됨(ex: --weights yolov5s.pt)\\n- Transfer Learning(전이학습)은 아래의 링크를 참고(Backbone만 고정, Backbone과 Head 모두 고정 옵션을 제공)\\n\\n-- batch-size: 배치 사이즈 값\\n- 컴퓨터 GPU 성능에 맞게 설정\\n\\n---cfg: yolo v5 아키텍쳐 yaml 파일 경로\\n\\n- yolo v5는 s, m, l, x의 4가지 버전이 있음\\n- s가 가장 가벼운 모델\\n- x가 가장 무거운 모델\\n- 당연히 s가 성능이 제일 낮지만 FPS가 가장 높고, x가 성능이 제일 높지만 FPS는 가장 낮다.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-7) yolo v5 학습 진행 및 인자 설명\n",
    "\n",
    "\n",
    "# python train.py --data coco.yaml --cfg yolov5s.yaml --weights '' --batch-size 64\n",
    "#                                          yolov5m                                40\n",
    "#                                          yolov5l                                24\n",
    "#                                          yolov5x                                16\n",
    "\n",
    "'''\n",
    "--data: data yaml 파일 경로 (데이터셋 정보가 적힌 yaml 파일)\n",
    "--weights: Pre-Trained 모델 파일 경로 (pt 형식 파일)\n",
    "\n",
    "- 아무런 값을 적지 않으면('') 랜덤한 weight 값으로 초기화 및 학습 진행\n",
    "- (깃헙 공식 오피셜, pre-train을 추천)\n",
    "- pre-trained 파일이란? : 학습 파라미터(weight와 bias)가 잘 초기화된 파일\n",
    "- pre-trained를 할거면 위 링크에서 다운받은 weight file 이름을 적으면 됨(ex: --weights yolov5s.pt)\n",
    "- Transfer Learning(전이학습)은 아래의 링크를 참고(Backbone만 고정, Backbone과 Head 모두 고정 옵션을 제공)\n",
    "\n",
    "-- batch-size: 배치 사이즈 값\n",
    "- 컴퓨터 GPU 성능에 맞게 설정\n",
    "\n",
    "---cfg: yolo v5 아키텍쳐 yaml 파일 경로\n",
    "\n",
    "- yolo v5는 s, m, l, x의 4가지 버전이 있음\n",
    "- s가 가장 가벼운 모델\n",
    "- x가 가장 무거운 모델\n",
    "- 당연히 s가 성능이 제일 낮지만 FPS가 가장 높고, x가 성능이 제일 높지만 FPS는 가장 낮다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n- 해당 결과는 runs/detect/exp/ 위치에 저장됨\\n\\n# python detect.py --source data/images --weights yolov5s.pt --conf 0.25\\n\\n--source: 테스트 이미지(혹은 폴더) 경로\\n--weights: 학습이 완료된 weight 파일 경로(pt 형식)\\n\\n- train.py에서 weight 파일 저장 경로를 수정하지 않았다면,\\n~/yolov5/runs/train/exp/weights/경로에 best.pt와 last.pt가 있다.\\nbest.pt는 가장 성능이 좋은 weight이고, last.pt는 최신 weight파일이다.\\n\\n--conf: conf-threshold 값(0~1 사이의 값)\\n- class score가 설정한 값을 넘겨야, 바운딩 박스를 그림\\n\\n[Error : DistributionNotFound: The 'pycocotools>=2.0' distribution was not found and is required by the application]\\n\\n- detect.py를 실행시켰는데 위와 같은 에러가 나올 수 있다.\\n- 현재 pycocotools는 다운로드 에러가 발생하는데 왜 발생하는지 모르겠다.\\n- 학습할 때 pycocotools는 필요 없으니 line 167의 check_requirements()를 주석처리 해줌\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-8) 이미지로 테스트 해보기\n",
    "'''\n",
    "- 해당 결과는 runs/detect/exp/ 위치에 저장됨\n",
    "\n",
    "# python detect.py --source data/images --weights yolov5s.pt --conf 0.25\n",
    "\n",
    "--source: 테스트 이미지(혹은 폴더) 경로\n",
    "--weights: 학습이 완료된 weight 파일 경로(pt 형식)\n",
    "\n",
    "- train.py에서 weight 파일 저장 경로를 수정하지 않았다면,\n",
    "~/yolov5/runs/train/exp/weights/경로에 best.pt와 last.pt가 있다.\n",
    "best.pt는 가장 성능이 좋은 weight이고, last.pt는 최신 weight파일이다.\n",
    "\n",
    "--conf: conf-threshold 값(0~1 사이의 값)\n",
    "- class score가 설정한 값을 넘겨야, 바운딩 박스를 그림\n",
    "\n",
    "[Error : DistributionNotFound: The 'pycocotools>=2.0' distribution was not found and is required by the application]\n",
    "\n",
    "- detect.py를 실행시켰는데 위와 같은 에러가 나올 수 있다.\n",
    "- 현재 pycocotools는 다운로드 에러가 발생하는데 왜 발생하는지 모르겠다.\n",
    "- 학습할 때 pycocotools는 필요 없으니 line 167의 check_requirements()를 주석처리 해줌\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nyolo v5도 일반적인 object detection의 구성과 큰 차이점은 없다.\\n크게 Backbone과 Head 부분으로 구성됨\\n이 구성은 ~/yolov5/models/ 경로에 있는 yolov5s.yaml 파일 등에서 좀 더 자세히 확인할 수 있다.\\n먼저 전반적인 설명을 하고 나서, yolo v5의 레이어 모듈, 백본 그리고 헤드에 관해 설명\\n\\n- Backbone은 이미지로부터 Feature map을 추출하는 부분으로, CSP-Darknet을 사용\\nyolo v4의 백복과 유사함.\\nyolov3의 backbone은 darknet53으로 csp가 적용되지 않는다.\\n특이하게도 yolo v5의 backbone은 종류가 4가지나 됨\\n제일 작고 가벼운 yolo v5-s부터 m,l,x 까지 포함해서 총 4가지 버전이 있다.\\n\\nHead는 추출된 Feature map을 바탕으로 물체의 위치를 찾는 부분이다.\\n흔히 말하는 Anchor Box(Default Box)를 처음에 설정하고 이를 이용하여\\n최종적인 Bounding Box를 생성함\\nyolo v3와 동일하게 3가지의 scale에서 바운딩 박스를 생성함\\n(8 pixel 정보를 가진 작은 물체, 16pixel 정보를 가진 중간 물체,\\n32 pixel 정보를 가진 큰 물체를 인식 가능) 또한 각 스케일에서 3개의 앵커 박스를 사용함\\n그러므로 총 9개의 앵커 박스가 있다.\\n\\n4가지 버전 중 yolo v5-s 아키텍처 분석을 해보겠다.\\n아키텍처에 관한 코드는 ~/yolov5/models/경로에 있다.\\n2개의 코드가 중심이다.\\n(1) yolo.py: 욜로 아키텍처에 관한 코드이다. 이 코드를 통해 욜로 아키텍처가 생성됨\\n(2) common.py: 욜로 아키텍처를 구성하는 모듈(레이어)에 관한 코드이다. 이 코드에 conv, BottleneckCSP 등등 욜로 모듈들이 적혀있다.\\n\\nyolo.py를 실행시키면 아래와 같이 yolo v5 s의아키텍처 구조가 나온다.\\nyolov5s.yaml과 크게 다른 점은 없다.\\n(아마 경로상의 문제 때문에 yolo.py의 위치를 ~/yolov5/models/ 에서 ~/yolov5/로 옮겨야 작동이 될 것이다.)\\n\\n# Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)\\n#            device1 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)\\n\\n\\n#                  from  n    params  module                                  arguments                     \\n#   0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \\n#   1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \\n#   2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \\n#   3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \\n#   4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \\n#   5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \\n#   6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \\n#   7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \\n#   8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \\n#   9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \\n#  10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \\n#  11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \\n#  12           [-1, 6]  1         0  models.common.Concat                    [1]                           \\n#  13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \\n#  14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \\n#  15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \\n#  16           [-1, 4]  1         0  models.common.Concat                    [1]                           \\n#  17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \\n#  18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \\n#  19          [-1, 14]  1         0  models.common.Concat                    [1]                           \\n#  20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \\n#  21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \\n#  22          [-1, 10]  1         0  models.common.Concat                    [1]                           \\n#  23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \\n#  24      [17, 20, 23]  1    229245  Detect                                  [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\\n\\n# Model Summary: 191 layers, 7.46816e+06 parameters, 7.46816e+06 gradients\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. yolo v5 아키텍처 분석하기\n",
    "\n",
    "'''\n",
    "yolo v5도 일반적인 object detection의 구성과 큰 차이점은 없다.\n",
    "크게 Backbone과 Head 부분으로 구성됨\n",
    "이 구성은 ~/yolov5/models/ 경로에 있는 yolov5s.yaml 파일 등에서 좀 더 자세히 확인할 수 있다.\n",
    "먼저 전반적인 설명을 하고 나서, yolo v5의 레이어 모듈, 백본 그리고 헤드에 관해 설명\n",
    "\n",
    "- Backbone은 이미지로부터 Feature map을 추출하는 부분으로, CSP-Darknet을 사용\n",
    "yolo v4의 백복과 유사함.\n",
    "yolov3의 backbone은 darknet53으로 csp가 적용되지 않는다.\n",
    "특이하게도 yolo v5의 backbone은 종류가 4가지나 됨\n",
    "제일 작고 가벼운 yolo v5-s부터 m,l,x 까지 포함해서 총 4가지 버전이 있다.\n",
    "\n",
    "Head는 추출된 Feature map을 바탕으로 물체의 위치를 찾는 부분이다.\n",
    "흔히 말하는 Anchor Box(Default Box)를 처음에 설정하고 이를 이용하여\n",
    "최종적인 Bounding Box를 생성함\n",
    "yolo v3와 동일하게 3가지의 scale에서 바운딩 박스를 생성함\n",
    "(8 pixel 정보를 가진 작은 물체, 16pixel 정보를 가진 중간 물체,\n",
    "32 pixel 정보를 가진 큰 물체를 인식 가능) 또한 각 스케일에서 3개의 앵커 박스를 사용함\n",
    "그러므로 총 9개의 앵커 박스가 있다.\n",
    "\n",
    "4가지 버전 중 yolo v5-s 아키텍처 분석을 해보겠다.\n",
    "아키텍처에 관한 코드는 ~/yolov5/models/경로에 있다.\n",
    "2개의 코드가 중심이다.\n",
    "(1) yolo.py: 욜로 아키텍처에 관한 코드이다. 이 코드를 통해 욜로 아키텍처가 생성됨\n",
    "(2) common.py: 욜로 아키텍처를 구성하는 모듈(레이어)에 관한 코드이다. 이 코드에 conv, BottleneckCSP 등등 욜로 모듈들이 적혀있다.\n",
    "\n",
    "yolo.py를 실행시키면 아래와 같이 yolo v5 s의아키텍처 구조가 나온다.\n",
    "yolov5s.yaml과 크게 다른 점은 없다.\n",
    "(아마 경로상의 문제 때문에 yolo.py의 위치를 ~/yolov5/models/ 에서 ~/yolov5/로 옮겨야 작동이 될 것이다.)\n",
    "\n",
    "# Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)\n",
    "#            device1 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)\n",
    "\n",
    "\n",
    "#                  from  n    params  module                                  arguments                     \n",
    "#   0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
    "#   1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
    "#   2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
    "#   3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
    "#   4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
    "#   5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
    "#   6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
    "#   7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
    "#   8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
    "#   9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
    "#  10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
    "#  11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
    "#  12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
    "#  13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
    "#  14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
    "#  15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
    "#  16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
    "#  17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
    "#  18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
    "#  19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
    "#  20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
    "#  21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
    "#  22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
    "#  23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
    "#  24      [17, 20, 23]  1    229245  Detect                                  [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
    "\n",
    "# Model Summary: 191 layers, 7.46816e+06 parameters, 7.46816e+06 gradients\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n우선 각 모듈에 관한 설명을 하겠다.\\ncommon.py를 기반으로 작성한다.\\n각 모듈 클래스의 forwqrd 함수를 기반으로 분석했다.\\n\\n--Focus: 이 모듈은 아직 이해하지 못했다. 주석에 따르면 x(b,c,w,h) -> y(b,4c,w/2,h/2)과\\n같이 값이 변화한다고 하는데, 변수들(b,c,w,h 등)이 무엇을 의미하는지 모르겠다.\\nb: batch_size, c: channel, w: width, h:height를 의미\\n\\n# class Focus(nn.Module):\\n#     # Focus wh information into c-space\\n#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\\n#         super(Focus, self).__init__()\\n#         self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\\n\\n#     def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\\n\\n--Conv: 일반적인 conv + batch_norm 레이어이다.\\nforward 함수를 보면, 이 레이어는 conv 연산을 한 후에 batch Normalization 과정을 거쳐줌\\n활성화 함수로는 Hard swish 함수를 사용함\\n\\n# class Conv(nn.Module):\\n#     # Standard convolution\\n#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\\n#         super(Conv, self).__init__()\\n#         self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\\n#         self.bn = nn.BatchNorm2d(c2)\\n#         self.act = nn.Hardswish() if act else nn.Identity()\\n\\n#     def forward(self, x):\\n#         return self.act(self.bn(self.conv(x)))\\n\\n#     def fuseforward(self, x):\\n#         return self.act(self.conv(x))\\n\\n-- Bottleneck\\n이 모듈은 ResNet에서 동일하게 사용한 Short-cut Connection이 있는 블록이다.\\n아래의 그림과 같은 구조이다.\\n\\n# class Bottleneck(nn.Module):\\n#     # Standard bottleneck\\n#     def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\\n#         super(Bottleneck, self).__init__()\\n#         c_ = int(c2 * e)  # hidden channels\\n#         self.cv1 = Conv(c1, c_, 1, 1)\\n#         self.cv2 = Conv(c_, c2, 3, 1, g=g)\\n#         self.add = shortcut and c1 == c2\\n\\n#     def forward(self, x):\\n#         return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\\n\\n--BottleneckCSP\\n이 모듈은 yolo v5의 핵심이다. 여기에서는 4개의 conv layer가 생성됨\\n- conv1, conv4: conv + batch_norm 레이어\\n- conv2, conv3: conv 레이어 (batch_norm 적용 x)\\n\\n그 다음 CSP 구조 답게 2개의 y값 생성을 함\\n- y1: Short-Connection으로 연결된 conv1 -> conv3 연산 값\\n- y2: 단순히 conv2를 연산한 값\\n\\n마지막으로 y1과 y2 값을 합치고, conv4 레이어를 통과하여 연산을 합니다.\\n# class BottleneckCSP(nn.Module):\\n#     # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\\n#     def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\\n#         super(BottleneckCSP, self).__init__()\\n#         c_ = int(c2 * e)  # hidden channels\\n#         self.cv1 = Conv(c1, c_, 1, 1)\\n#         self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\\n#         self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\\n#         self.cv4 = Conv(2 * c_, c2, 1, 1)\\n#         self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\\n#         self.act = nn.LeakyReLU(0.1, inplace=True)\\n#         self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\\n\\n#     def forward(self, x):\\n#         y1 = self.cv3(self.m(self.cv1(x)))\\n#         y2 = self.cv2(x)\\n#         return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\\n\\n--SPP\\n이 모듈은 yolov3-SPP에서 사용했던 Spatial Pyramid Pooling Layer 입니다.\\nspatial bins 으로 5*5, 9*9, 13*13 피처맵을 사용했으며,\\n최종적으로 5+9+13=27의 크기로 고정된 1차원 형태의 배열을 생성하여,\\nFully Connected Layer에서 입력으로 들어갈 수 있게 함\\n# class SPP(nn.Module):\\n#     # Spatial pyramid pooling layer used in YOLOv3-SPP\\n#     def __init__(self, c1, c2, k=(5, 9, 13)):\\n#         super(SPP, self).__init__()\\n#         c_ = c1 // 2  # hidden channels\\n#         self.cv1 = Conv(c1, c_, 1, 1)\\n#         self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\\n#         self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\\n\\n#     def forward(self, x):\\n#         x = self.cv1(x)\\n#         return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\\n\\n--Concat\\n이 모듈은 단순히 2개의 conv 레이어 연산 값을 합치는 것이라고 보면 됨\\n# class Concat(nn.Module):\\n#     # Concatenate a list of tensors along dimension\\n#     def __init__(self, dimension=1):\\n#         super(Concat, self).__init__()\\n#         self.d = dimension\\n\\n#     def forward(self, x):\\n#         return torch.cat(x, self.d)\\n\\n--torch.nn.modules.upsampling.Upsample\\n이 모듈도 단순히 업샘플링하는 토치의 기본 라이브러리 함수이다.\\n구조 값을 따르면 피쳐맵의 각 배열의 갯수를 2배로 올려줌\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1) yolo v5 s' Module\n",
    "'''\n",
    "우선 각 모듈에 관한 설명을 하겠다.\n",
    "common.py를 기반으로 작성한다.\n",
    "각 모듈 클래스의 forwqrd 함수를 기반으로 분석했다.\n",
    "\n",
    "--Focus: 이 모듈은 아직 이해하지 못했다. 주석에 따르면 x(b,c,w,h) -> y(b,4c,w/2,h/2)과\n",
    "같이 값이 변화한다고 하는데, 변수들(b,c,w,h 등)이 무엇을 의미하는지 모르겠다.\n",
    "b: batch_size, c: channel, w: width, h:height를 의미\n",
    "\n",
    "# class Focus(nn.Module):\n",
    "#     # Focus wh information into c-space\n",
    "#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
    "#         super(Focus, self).__init__()\n",
    "#         self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n",
    "\n",
    "#     def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n",
    "\n",
    "--Conv: 일반적인 conv + batch_norm 레이어이다.\n",
    "forward 함수를 보면, 이 레이어는 conv 연산을 한 후에 batch Normalization 과정을 거쳐줌\n",
    "활성화 함수로는 Hard swish 함수를 사용함\n",
    "\n",
    "# class Conv(nn.Module):\n",
    "#     # Standard convolution\n",
    "#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
    "#         super(Conv, self).__init__()\n",
    "#         self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "#         self.bn = nn.BatchNorm2d(c2)\n",
    "#         self.act = nn.Hardswish() if act else nn.Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "#     def fuseforward(self, x):\n",
    "#         return self.act(self.conv(x))\n",
    "\n",
    "-- Bottleneck\n",
    "이 모듈은 ResNet에서 동일하게 사용한 Short-cut Connection이 있는 블록이다.\n",
    "아래의 그림과 같은 구조이다.\n",
    "\n",
    "# class Bottleneck(nn.Module):\n",
    "#     # Standard bottleneck\n",
    "#     def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n",
    "#         super(Bottleneck, self).__init__()\n",
    "#         c_ = int(c2 * e)  # hidden channels\n",
    "#         self.cv1 = Conv(c1, c_, 1, 1)\n",
    "#         self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
    "#         self.add = shortcut and c1 == c2\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "--BottleneckCSP\n",
    "이 모듈은 yolo v5의 핵심이다. 여기에서는 4개의 conv layer가 생성됨\n",
    "- conv1, conv4: conv + batch_norm 레이어\n",
    "- conv2, conv3: conv 레이어 (batch_norm 적용 x)\n",
    "\n",
    "그 다음 CSP 구조 답게 2개의 y값 생성을 함\n",
    "- y1: Short-Connection으로 연결된 conv1 -> conv3 연산 값\n",
    "- y2: 단순히 conv2를 연산한 값\n",
    "\n",
    "마지막으로 y1과 y2 값을 합치고, conv4 레이어를 통과하여 연산을 합니다.\n",
    "# class BottleneckCSP(nn.Module):\n",
    "#     # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n",
    "#     def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "#         super(BottleneckCSP, self).__init__()\n",
    "#         c_ = int(c2 * e)  # hidden channels\n",
    "#         self.cv1 = Conv(c1, c_, 1, 1)\n",
    "#         self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n",
    "#         self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n",
    "#         self.cv4 = Conv(2 * c_, c2, 1, 1)\n",
    "#         self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n",
    "#         self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "#         self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y1 = self.cv3(self.m(self.cv1(x)))\n",
    "#         y2 = self.cv2(x)\n",
    "#         return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n",
    "\n",
    "--SPP\n",
    "이 모듈은 yolov3-SPP에서 사용했던 Spatial Pyramid Pooling Layer 입니다.\n",
    "spatial bins 으로 5*5, 9*9, 13*13 피처맵을 사용했으며,\n",
    "최종적으로 5+9+13=27의 크기로 고정된 1차원 형태의 배열을 생성하여,\n",
    "Fully Connected Layer에서 입력으로 들어갈 수 있게 함\n",
    "# class SPP(nn.Module):\n",
    "#     # Spatial pyramid pooling layer used in YOLOv3-SPP\n",
    "#     def __init__(self, c1, c2, k=(5, 9, 13)):\n",
    "#         super(SPP, self).__init__()\n",
    "#         c_ = c1 // 2  # hidden channels\n",
    "#         self.cv1 = Conv(c1, c_, 1, 1)\n",
    "#         self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n",
    "#         self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.cv1(x)\n",
    "#         return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n",
    "\n",
    "--Concat\n",
    "이 모듈은 단순히 2개의 conv 레이어 연산 값을 합치는 것이라고 보면 됨\n",
    "# class Concat(nn.Module):\n",
    "#     # Concatenate a list of tensors along dimension\n",
    "#     def __init__(self, dimension=1):\n",
    "#         super(Concat, self).__init__()\n",
    "#         self.d = dimension\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.cat(x, self.d)\n",
    "\n",
    "--torch.nn.modules.upsampling.Upsample\n",
    "이 모듈도 단순히 업샘플링하는 토치의 기본 라이브러리 함수이다.\n",
    "구조 값을 따르면 피쳐맵의 각 배열의 갯수를 2배로 올려줌\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n특이하게도 yolo v5의 backbone 종류가 4가지나 됨\\ns(small), m(medium), l(large), x(extra인가?) 버전이 있고,\\ns가 backbone 크기가 가장 작고(=layers 수가 가장 적고) 빠르며,\\nx가 크기가 가장 크고 느리다.\\n\\nBackbone들은 2가지 변수로 결정됨\\n바로 yaml 파일에 있는 'depth_multiple'(model depth multiple)과\\n'width_multiple'(layer channel multiple)이다.\\n\\n당연히 yolo v5-s의 depth&width_multiple 이 가장 작고(depth_multiple:0.33,\\nwidth_multiple:0.50), x의 depth&width_multiple은 1.33, 12.5로 가장 크다.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2) yolo v5 s' backbone\n",
    "'''\n",
    "특이하게도 yolo v5의 backbone 종류가 4가지나 됨\n",
    "s(small), m(medium), l(large), x(extra인가?) 버전이 있고,\n",
    "s가 backbone 크기가 가장 작고(=layers 수가 가장 적고) 빠르며,\n",
    "x가 크기가 가장 크고 느리다.\n",
    "\n",
    "Backbone들은 2가지 변수로 결정됨\n",
    "바로 yaml 파일에 있는 'depth_multiple'(model depth multiple)과\n",
    "'width_multiple'(layer channel multiple)이다.\n",
    "\n",
    "당연히 yolo v5-s의 depth&width_multiple 이 가장 작고(depth_multiple:0.33,\n",
    "width_multiple:0.50), x의 depth&width_multiple은 1.33, 12.5로 가장 크다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n먼저 depth_multiple이 모델의 구조를 어떻게 변화시키는지 보겠다.\\n결론부터 얘기하자면, depth_multiple 값이 클수록 BottleneckCSP 모듈(레이어)이\\n더 많이 반복되어, 더 깊은 모델이 됩니다.\\n모든 설명은 yolo.py 코드를 기반으로 설명한다.\\n먼저 parse_model 함수를 봅시다. (함수 일부만 가져온다)\\n\\n# def parse_model(d, ch):  # model_dict, input_channels(3)\\n#     logger.info(\\'\\n%3s%18s%3s%10s  %-40s%-30s\\' % (\\'\\', \\'from\\', \\'n\\', \\'params\\', \\'module\\', \\'arguments\\'))\\n#     anchors, nc, gd, gw = d[\\'anchors\\'], d[\\'nc\\'], d[\\'depth_multiple\\'], d[\\'width_multiple\\']\\n#     # print(\"depth_multiple : %s\" %gd)\\n#     na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\\n#     no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\\n\\n위의 코드를 보면, yaml 파일에서 읽어온 depth_multiple 값이 gd라는 변수에 저장되는 것을 볼 수 있다.\\n(저와 같이 print문을 활용해서 변수 값들이 어떻게 출력되는지 보시면,\\n구조를 이해하는데 매우 좋다.)\\n이제 변수 gd는 depth gain의 변수인 n을 구하는데에 사용됨\\n\\n# layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\\n#     for i, (f, n, m, args) in enumerate(d[\\'backbone\\'] + d[\\'head\\']):  # from, number, module, args\\n#         m = eval(m) if isinstance(m, str) else m  # eval strings\\n#         for j, a in enumerate(args):\\n#             try:\\n#                 args[j] = eval(a) if isinstance(a, str) else a  # eval strings\\n#             except:\\n#                 pass\\n\\n#         n = max(round(n * gd), 1) if n > 1 else n  # depth gain\\n#         # print(\"depth_gain : %s\" %n)\\n\\ndepth gain의 변수 n을 계산하기 위해서 2개의 변수를 사용함\\n위에서 설명한 gd(=depth_multiple)값과 yaml 파일에서 number라고 적혀있는 n값을 사용함\\nn(number) 값을 설명하기 위해서 yolov5s.yaml 파일을 보겠습니다.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2-(1) Depth_Multiple\n",
    "\n",
    "'''\n",
    "먼저 depth_multiple이 모델의 구조를 어떻게 변화시키는지 보겠다.\n",
    "결론부터 얘기하자면, depth_multiple 값이 클수록 BottleneckCSP 모듈(레이어)이\n",
    "더 많이 반복되어, 더 깊은 모델이 됩니다.\n",
    "모든 설명은 yolo.py 코드를 기반으로 설명한다.\n",
    "먼저 parse_model 함수를 봅시다. (함수 일부만 가져온다)\n",
    "\n",
    "# def parse_model(d, ch):  # model_dict, input_channels(3)\n",
    "#     logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n",
    "#     anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n",
    "#     # print(\"depth_multiple : %s\" %gd)\n",
    "#     na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n",
    "#     no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n",
    "\n",
    "위의 코드를 보면, yaml 파일에서 읽어온 depth_multiple 값이 gd라는 변수에 저장되는 것을 볼 수 있다.\n",
    "(저와 같이 print문을 활용해서 변수 값들이 어떻게 출력되는지 보시면,\n",
    "구조를 이해하는데 매우 좋다.)\n",
    "이제 변수 gd는 depth gain의 변수인 n을 구하는데에 사용됨\n",
    "\n",
    "# layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
    "#     for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n",
    "#         m = eval(m) if isinstance(m, str) else m  # eval strings\n",
    "#         for j, a in enumerate(args):\n",
    "#             try:\n",
    "#                 args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#         n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n",
    "#         # print(\"depth_gain : %s\" %n)\n",
    "\n",
    "depth gain의 변수 n을 계산하기 위해서 2개의 변수를 사용함\n",
    "위에서 설명한 gd(=depth_multiple)값과 yaml 파일에서 number라고 적혀있는 n값을 사용함\n",
    "n(number) 값을 설명하기 위해서 yolov5s.yaml 파일을 보겠습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\pytorch38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 연구원님 과제 얼굴 박스치기?\n",
    "\n",
    "# import necessary packages\n",
    "import cvlib as cv\n",
    "import cv2\n",
    "from yolov5facedetector.face_detector import YoloDetector\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model = YoloDetector(target_size=1080,gpu=0,min_face=0)\n",
    "\n",
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    " \n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    " \n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "    \n",
    "    frame=cv2.resize(frame, dsize=(0,0), fx=0.7, fy=0.7, interpolation=cv2.INTER_CUBIC)\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    bboxes, confs, points = model.predict(frame)\n",
    "\n",
    "    if not status:\n",
    "        print(\"Could not read frame\")\n",
    "        exit()\n",
    " \n",
    "    for idx in range(len(bboxes[0])):\n",
    "\n",
    "        (startX, startY)=bboxes[0][idx][0], bboxes[0][idx][1]\n",
    "        (endX, endY)=bboxes[0][idx][2], bboxes[0][idx][3]\n",
    "    \n",
    "        # draw rectangle over face\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    # display output\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow(\"Real-time face detection\", frame)\n",
    " \n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# release resources\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연구원님 과제 얼굴 모자이크?\n",
    "\n",
    "\n",
    "# import necessary packages\n",
    "import cvlib as cv\n",
    "import cv2\n",
    "from yolov5facedetector.face_detector import YoloDetector\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model = YoloDetector(target_size=1080,gpu=0,min_face=0)\n",
    "\n",
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    " \n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    " \n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "    \n",
    "    frame=cv2.resize(frame, dsize=(0,0), fx=0.7, fy=0.7, interpolation=cv2.INTER_CUBIC)\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    bboxes, confs, points = model.predict(frame)\n",
    "\n",
    "    if not status:\n",
    "        print(\"Could not read frame\")\n",
    "        exit()\n",
    " \n",
    "    for idx in range(len(bboxes[0])):\n",
    "\n",
    "        (startX, startY)=bboxes[0][idx][0], bboxes[0][idx][1]\n",
    "        (endX, endY)=bboxes[0][idx][2], bboxes[0][idx][3]\n",
    "    \n",
    "        # 모자이크 처리\n",
    "        face_region = frame[startY:endY, startX:endX]\n",
    "        \n",
    "        M = face_region.shape[0]\n",
    "        N = face_region.shape[1]\n",
    " \n",
    "        face_region = cv2.resize(face_region, None, fx=0.05, fy=0.05, interpolation=cv2.INTER_AREA)\n",
    "        face_region = cv2.resize(face_region, (N, M), interpolation=cv2.INTER_AREA)\n",
    "        frame[startY:endY, startX:endX] = face_region\n",
    "\n",
    "    # display output\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow(\"Real-time face detection\", frame)\n",
    " \n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# release resources\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\pytorch38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 합치기\n",
    "\n",
    "# import necessary packages\n",
    "import cvlib as cv\n",
    "import cv2\n",
    "from yolov5facedetector.face_detector import YoloDetector\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model = YoloDetector(target_size=1080,gpu=0,min_face=0)\n",
    "\n",
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    " \n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    " \n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "    \n",
    "    frame=cv2.resize(frame, dsize=(0,0), fx=0.7, fy=0.7, interpolation=cv2.INTER_CUBIC)\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    bboxes, confs, points = model.predict(frame)\n",
    "\n",
    "    if not status:\n",
    "        print(\"Could not read frame\")\n",
    "        exit()\n",
    " \n",
    "    for idx in range(len(bboxes[0])):\n",
    "\n",
    "        (startX, startY)=bboxes[0][idx][0], bboxes[0][idx][1]\n",
    "        (endX, endY)=bboxes[0][idx][2], bboxes[0][idx][3]\n",
    "\n",
    "        key = cv2.waitKey(33)\n",
    "\n",
    "        if key == 26:\n",
    "            # draw rectangle over face\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        if key == 24:\n",
    "\n",
    "            # 모자이크 처리\n",
    "            face_region = frame[startY:endY, startX:endX]\n",
    "            \n",
    "            M = face_region.shape[0]\n",
    "            N = face_region.shape[1]\n",
    "    \n",
    "            face_region = cv2.resize(face_region, None, fx=0.05, fy=0.05, interpolation=cv2.INTER_AREA)\n",
    "            face_region = cv2.resize(face_region, (N, M), interpolation=cv2.INTER_AREA)\n",
    "            frame[startY:endY, startX:endX] = face_region\n",
    "\n",
    "            \n",
    "\n",
    "    # display output\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow(\"Real-time face detection\", frame)\n",
    " \n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# release resources\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e713db361fd0c0a151521921a307db5316dc01e9b741659f15d8bea985d071ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
